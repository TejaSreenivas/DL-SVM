{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3a672abb-a2c8-4d59-9c65-42ae2c89ac84",
    "_uuid": "098d097dea66b981144f3aaec875f1c824f57883"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "import os\n",
    "import platform\n",
    "from load_cifar import load_data\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "acab539e-99ab-49f8-aaa9-9c08678494ec",
    "_uuid": "62ff7f4915898a073659a4677eb70c4a6a9e6eb4"
   },
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "dbda6db8-29f1-4cff-9c48-24f35db15f0c",
    "_uuid": "79098de67d87caf443e057aba591eeb3985b7a37"
   },
   "outputs": [],
   "source": [
    "data = load_data('C:/Users/k_tej/Documents/TEJA/ML_resources/DL_projects/data_sets/cifar-10-batches-py')\n",
    "#data = load_data('/floyd/input/cifar_10_batches_py/cifar-10-batches-py')\n",
    "data=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "0c272f85-bd27-4eeb-99a7-6fd036e99344",
    "_uuid": "53b8f7f8fb884a4c8f18350c1d4626cf198bf3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 32, 32, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast ufunc subtract output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-71b882ebb4e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Invoke the above function to get our data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train data shape: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train labels shape: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-71b882ebb4e6>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[1;34m(num_training, num_validation, num_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Normalize the data: subtract the mean image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmean_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mmean_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mmean_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mmean_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast ufunc subtract output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    X_train, y_train, X_test, y_test = data['train_x'].astype(np.float64),data['train_y'].astype(np.float64)\n",
    "    X_test, y_test = data['test_x'].astype(np.float64),data['test_y'].astype(np.float64)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    print(X_train.shape)\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0679fb2a-62be-415b-9d0c-cd901db6eaf3",
    "_uuid": "e72e7c97da89f05d73af6932e182c82be0c9b00f"
   },
   "outputs": [],
   "source": [
    "# define net\n",
    "class CifarNet():\n",
    "    def __init__(self):\n",
    "        # conv layer\n",
    "        # H2 = (H1 - F + 2P)/S +1\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x32 = 25088\n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[5, 5, 3, 32])\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[5, 5, 32, 64])\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[3136, 1024])\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[1024, 10])\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "        # conv2d\n",
    "        # ReLu\n",
    "        # conv2d\n",
    "        # ReLu\n",
    "        # maxpool\n",
    "        # Batch Norm\n",
    "        # Affine\n",
    "        # Batch Norm\n",
    "        # ReLu\n",
    "        # Affine\n",
    "        # dropout\n",
    "        # Batch Norm\n",
    "\n",
    "        # conv layer\n",
    "        # H2 = (H1 - F + 2P)/S +1\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x32 = 25088\n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "\n",
    "        # define our graph (e.g. two_layer_convnet) with stride 1\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        print(conv1.shape)\n",
    "        # ReLU Activation Layer\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        print(relu1)\n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 2, 2, 1], padding='VALID') + self.bconv2\n",
    "        print(conv2.shape)\n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        print(relu2)\n",
    "        # 2x2 Max Pooling layer with a stride of 2\n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2), strides=2)\n",
    "        print(maxpool.shape)\n",
    "        maxpool_flat = tf.reshape(maxpool,[-1,3136])\n",
    "        # Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n",
    "        bn1 = tf.layers.batch_normalization(inputs=maxpool_flat, center=True, scale=True, training=is_training)\n",
    "        # Affine layer with 1024 output units\n",
    "        affine1 = tf.matmul(bn1, self.W1) + self.b1\n",
    "        print(affine1.shape)\n",
    "        # vanilla batch normalization\n",
    "        affine1_flat = tf.reshape(affine1,[-1,1024])\n",
    "        bn2 = tf.layers.batch_normalization(inputs=affine1, center=True, scale=True, training=is_training)\n",
    "        print(bn2.shape)\n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(bn2)\n",
    "        print(relu2.shape)\n",
    "        # dropout\n",
    "        drop1 = tf.layers.dropout(inputs=relu2, training=is_training)\n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop1, self.W2) + self.b2\n",
    "        # vanilla batch normalization\n",
    "        affine2_flat = tf.reshape(affine2,[-1,3136])\n",
    "        self.predict = tf.layers.batch_normalization(inputs=affine2, center=True, scale=True, training=is_training)\n",
    "        print(self.predict.shape)\n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,\n",
    "                  epochs=1, batch_size=64, print_every=100,\n",
    "                  training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "        if isSoftMax:\n",
    "            correct_prediction = tf.nn.softmax(self.predict)\n",
    "        else:\n",
    "            correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        training_now = training is not None\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        if training_now:\n",
    "            variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: training_now }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if training_now and (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "                  .format(total_loss,total_correct,e+1))\n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69643b48-526b-4c32-af3e-021d44c54023",
    "_uuid": "53a027e508704650f9350bcae18cd5a9bf78014a"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = CifarNet()\n",
    "net.forward(X,y,is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "095a98f0-4822-4ed3-b44e-f290cc8b65a9",
    "_uuid": "494f291d11d390c0901ad2c5997be2df1d575381"
   },
   "outputs": [],
   "source": [
    "# Annealing the learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-3\n",
    "end_learning_rate = 5e-3\n",
    "decay_steps = 10000\n",
    "\n",
    "learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "                                          decay_steps, end_learning_rate,\n",
    "                                          power=0.5)\n",
    "\n",
    "exp_learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               100000, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "# Feel free to play with this cell\n",
    "mean_loss = None\n",
    "optimizer = None\n",
    "\n",
    "# define our loss\n",
    "cross_entr_loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=net.predict)\n",
    "mean_loss = tf.reduce_mean(cross_entr_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(exp_learning_rate)\n",
    "\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d2d71560-1af3-4eed-9ab7-f2e992fdd0f3",
    "_uuid": "d0bc3aca85023f3dfb4883d3205a2fdc78dbff22"
   },
   "outputs": [],
   "source": [
    "# train with 10 epochs\n",
    "sess = tf.Session()\n",
    "\n",
    "try:\n",
    "    with tf.device(\"/cpu:0\") as dev:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        net.run(sess, mean_loss, X_train, y_train, 10, 64, 200, train_step, True)\n",
    "        print('Validation')\n",
    "        net.run(sess, mean_loss, X_val, y_val, 1, 64)\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc907e4d-0ed7-4fdc-a726-cec27800948d",
    "_uuid": "fffadbfd91162500d980cd79d636e06ccc0cd3a6"
   },
   "outputs": [],
   "source": [
    "# view net model result on train  and validation set\n",
    "print('Training')\n",
    "net.run(sess, mean_loss, X_train, y_train, 1, 64)\n",
    "print('Validation')\n",
    "net.run(sess, mean_loss, X_val, y_val, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49fe1733-ec19-4919-a76f-b4260a114cd0",
    "_uuid": "685c202406f21105eb173eb92f4cc22dbe7c476f"
   },
   "outputs": [],
   "source": [
    "# check result on test\n",
    "print('Test')\n",
    "net.run(sess, mean_loss, X_test, y_test, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3bfa901e-67d1-4ca9-aa90-bc4ff6fdc0ff",
    "_uuid": "e242183c55e18297d3ba816bad22a2cc09190b62"
   },
   "outputs": [],
   "source": [
    "# create a feed dictionary for this batch\n",
    "feed_dict = {X: X_test,\n",
    "             y: y_test,\n",
    "             is_training: False}\n",
    "\n",
    "# predict\n",
    "predict = sess.run(tf.nn.softmax(net.predict), feed_dict=feed_dict)\n",
    "predict_df = pd.DataFrame(predict, columns=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f121fa99-bbc8-4e7d-aa06-41e712d781cb",
    "_uuid": "f28553b32154ae1186e7e050dbf308da4630e2af"
   },
   "outputs": [],
   "source": [
    "predict_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
